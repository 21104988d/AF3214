{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 6: Requests, BeautifulSoup, and API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requests and BeautifulSoup\n",
    "\n",
    "The requests library is the de facto standard for making HTTP request in Python. It abstracts the complexities of making requests behind a beautiful, simple API so that you can focus on interacting with services and consumming data in your application. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 1: SEC EDGAR\n",
    "\n",
    "We will be processing financial statements from the SEC. The SEC provides a database that allows you to access every single filing of listed companies in HTML format. \n",
    "\n",
    "https://www.sec.gov/edgar/searchedgar/companysearch.html\n",
    "\n",
    "Although you can apply these methods to any web page, we will concentrate on processing accoutning and finance data. \n",
    "\n",
    "Not all the filings are easily processed. There will be variations between companies. Therefore it will take **trial and error** to receive the information that you want to receive. \n",
    "\n",
    "The example below is more of an overview. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before we begin\n",
    "\n",
    "Often time when you are requesting data from the internet using a program, the website knows that it is a program that is making a request. \n",
    "\n",
    "One way to \"act like a human\" in your program is to change the request header. \n",
    "\n",
    "The usual request header when it is unchanged from a python program is like this: \n",
    "\n",
    "Accept-Encoding identity\n",
    "\n",
    "User-Agent Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0.0.0 Safari/537.36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing the header of the request\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36'}\n",
    "\n",
    "print (headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many ways for websites to check if the request is from a human, but based on the HTTP headers, the only setting that really mattters is the User-Agent.\n",
    "\n",
    "There are many settings within the header that you can further explore such as the Accept-Language header where you can possbily get the website in a different langauge. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will be using the following URL. \n",
    "\n",
    "url = 'https://www.sec.gov/Archives/edgar/data/1318605/000162828025003063/0001628280-25-003063.txt'\n",
    "\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "# Make up an error URL\n",
    "errorurl = 'https://www.amazon.com/404' \n",
    "\n",
    "# Will be used later as an example\n",
    "error_response = requests.get(errorurl, headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Error 404 \"Page not found\" is the error page displayed whenever someone asks for a page that’s simply not available on your site. \n",
    "# The reason for this is that there may be a link on your site that was wrong or the page might have been recently removed from the site. \n",
    "# As there is no web page to display, the web server sends a page that simply says \"404 Page not found\".\n",
    "print (error_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "response.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_response.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So with this status code information you can create an error check, so that when you are going through a list of urls you will know which ones are not valid. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if response.status_code == 200:\n",
    "    print('Success!')\n",
    "elif response.status_code == 404: \n",
    "    print('Not Found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if error_response.status_code ==200:\n",
    "    print('Success!')\n",
    "elif error_response.status_code == 404: \n",
    "    print('Not Found')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But if you noticed earlier, when we printed response we already got a 200 code because it was sucessful. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if response:\n",
    "    print('Success!')\n",
    "else:\n",
    "    print('An error has occurred.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if error_response:\n",
    "    print('Success!')\n",
    "else:\n",
    "    print('An error has occurred.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know that the response request was successful we can view the payload or the body of the request. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This allows you to view the raw bytes(b').\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Often you want to convert it into text encoding \n",
    "response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are some other useful methods that you could use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now let's save the body to a file so we don't have to constantly request it\n",
    "# This file appears in the same folder as this Jupyter file.\n",
    "filename = 'Tesla_10K_20241231.html'\n",
    "outputfile = open(filename, 'w', encoding = \"utf-8\")\n",
    "outputfile.write(response.text)\n",
    "outputfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the file, let's process it using Beautiful Soup 4.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "#https://www.w3schools.com/html/html_intro.asp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input the file that we just saved\n",
    "inputfile = open(filename,'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(inputfile)\n",
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's Clean up the HTML \n",
    "print (soup.prettify())\n",
    "# soup.prettify(): print in html tag level format--it looks very comfortable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try to look at some methods in beautiful soup.\n",
    "\n",
    "To use Beauiful Soup, you will have to understand the structure of an HTML file. The easiest structures to manipulate are usually well defined, but in the real world that is usually not the case. \n",
    "\n",
    "That is why, when you are mining for data, it is often a case of trial and error. \n",
    "\n",
    "You have to look for patterns that you could easily exploit and make rules for. \n",
    "\n",
    "Using Beautiful Soup is perfect when the html follows the rules that are defined for HTML documents, but as stated before this isn't usually the case.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will give you the title tag. We could use this for any of the tag pairs in the document.\n",
    "soup.title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When you are navigating through the document, the simpliest way is to use the tag as shown below, but the downside of this is that using the tag name will only give you the first tag by that name. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.title.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.title.parent.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <p> tag defines a paragraph.\n",
    "soup.p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# <a> tag defines a hyperlink, which is used to link from one page to another.\n",
    "soup.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List of the children are availible in contents\n",
    "soup.body.contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(soup.body.contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  In order to look for the other tags, you will have to use the find_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all <a> tags\n",
    "anchor_tags = soup.find_all('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_tags[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_tags[1].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_tags[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_tags[3]['href']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "href:\n",
    "\n",
    "(Hypertext REFerence) The HTML code used to create a link to another page. The HREF is an attribute of the anchor \n",
    "tag, which is also used to identify sections within a document. The HREF contains two components: the URL, which \n",
    "is the actual link, and the clickable text that appears on the page, called the \"anchor text.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If you know the id or a specific href, you could use the find function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find(href='#mda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An HTML table consists of one <table> element and one or more <tr>, <th>, and <td> elements.\n",
    "\n",
    "# The <tr> element defines a table row, the <th> element defines a table header, and the <td> element defines a table cell.\n",
    "\n",
    "# An HTML table may also include <caption>, <colgroup>, <thead>, <tfoot>, and <tbody> elements.\n",
    "\n",
    "html_tables = soup.find_all('table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "html_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.body.get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 2: S&P 500 List\n",
    "\n",
    "As you can see it can be quite difficult to work with the SEC filings due to their nature. There is another skillset that is required when processing them, and that is Natural Language Processing and Regular expressions. \n",
    "\n",
    "Let's try to extract data from something with more structure. \n",
    "\n",
    "We will try to extract the list of stocks in the S&P 500. \n",
    "Sp500Wiki = https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sp500Wiki = 'https://en.wikipedia.org/wiki/List_of_S%26P_500_companies'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's grab the HTML just like above\n",
    "\n",
    "spResponse = requests.get(Sp500Wiki, headers = headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's do some error checking\n",
    "if spResponse.status_code==200:\n",
    "    print('Success!')\n",
    "else:\n",
    "    print('An error has occurred.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's save the file to our local directory so we don't have to keep requesting data.\n",
    "sp500filename = 'wikiSP500.html'\n",
    "\n",
    "#Open the Stream \n",
    "outputfile = open(sp500filename, 'w', encoding = \"utf-8\")\n",
    "\n",
    "#Write to the file\n",
    "outputfile.write(spResponse.text)\n",
    "\n",
    "#Close the file\n",
    "outputfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are we ran into an encoding Error? You will run into quite often. So let's check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spResponse.encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spResponse.encoding = 'utf-8'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Open the Stream \n",
    "outputfile = open(sp500filename, 'w', encoding=\"utf-8\")\n",
    "\n",
    "#Write to the file\n",
    "outputfile.write(spResponse.text)\n",
    "\n",
    "#Close the file\n",
    "outputfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now file is saved, let's load it into soup, You can choose to load in from file or the response object. \n",
    "\n",
    "#You could use the file if you do not have internet access to make the request again. \n",
    "soup = BeautifulSoup(spResponse.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now let's take this opportunity to look at how the file is formatted. \n",
    "\n",
    "What do you see? \n",
    "\n",
    "You will notice that all the symbols are in a neat table so let's try. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = soup.find_all('tr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#What do you see below? \n",
    "rows[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#So you notice that symbol is the first element between the td so you try below\n",
    "rows[1].td"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#So now you see that it is between anchor tags, so try \n",
    "rows[1].td.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now you want the text between the tags\n",
    "rows[1].td.a.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now you can do it for 1 of the stocks in the S&P500, so how would you get them into a list?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's initialize a list first\n",
    "sp500Stocks = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's check the size of the rows list. \n",
    "len(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows[623]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That doesn't seem right why are there more than 500 rows? So look at the data again.\n",
    "\n",
    "There are two tables in the html so you have to be more specific with your find all. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp500_rows = soup.table.find_all('tr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sp500_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#That seems a little better, let's see why we have 504 elements\n",
    "sp500_rows[503]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's try to extract what we can\n",
    "\n",
    "for row in sp500_rows:\n",
    "    if row.td == None:\n",
    "        continue\n",
    "    else:\n",
    "        print (row.td.a.text)\n",
    "        sp500Stocks.append(row.td.a.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sp500Stocks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite being referred to as “500,” there are actually 503 listed stocks because a methodology change can lead to more companies having multiple listings in the index due to multiple share classes. For instance, Google’s parent company Alphabet has Class A and Class C shares.\n",
    "\n",
    "Today, the S&P 500 index covers approximately 80 percent of available market capitalization (or the total dollar market value of all listed firms’ outstanding shares)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using APIs with Python\n",
    "\n",
    "Many times there is a library written out that allows you to easily connect to an API. There are libaries for Bloomberg API and many others availible for you to install. \n",
    "\n",
    "We will be utilizing a free stock data API known as AlphaVantage. \n",
    "\n",
    "Please take the time to sign up at alphaVantage to get your API key below. \n",
    "\n",
    "https://www.alphavantage.co/\n",
    "\n",
    "We will be utilizing the wrapper \n",
    "https://github.com/RomelTorres/alpha_vantage\n",
    "\n",
    "please install with pip install alpha_vantage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 1: AlphaVantage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install alpha_vantage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#So let's import the library in which we will be using the timeseries module \n",
    "\n",
    "from alpha_vantage.timeseries import TimeSeries\n",
    "\n",
    "ts = TimeSeries(key='HNQ62X7XN1BICIIA', output_format='pandas')\n",
    "data = ts.get_intraday(symbol='DIA', outputsize='full') # DIA: Dow Jones Industrial Average ETF\n",
    "# outputsize='compact' requests data for last 100 days, and outputsize='full' requests data for the whole history\n",
    "\n",
    "# The adjusted closing price amends a stock's closing price to reflect that stock's value after accounting for \n",
    "# any corporate actions, such as stock splits, dividends, and rights offerings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, meta_data = ts.get_intraday(symbol='MSFT',interval='1min', outputsize='full')\n",
    "# meta data: a set of data that describes and gives information about other data\n",
    "# There are two parts in the API response: \"Meta Data\" and \"Time Series\"\n",
    "# The library is mapping meta_data to \"Meta Data\" and \"Time Series\" to data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, meta_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data, meta_data = ts.get_intraday(symbol='MSFT', outputsize='full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, meta_data = ts.get_intraday(symbol='MSFT', outputsize='compact')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now let's quickly save this to csv \n",
    "filename = 'microsoft.csv'\n",
    "\n",
    "data.to_csv(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's say we want to grab data for mutiple stocks. We don't have to always request the API especially since it is free, there is always a limitiation on how fast you can download files. So what do you do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We have a list of the following 5 stocks, let's just take 5 stocks from our sp500 list\n",
    "\n",
    "data_request = []\n",
    "for i in range(0,5):\n",
    "    data_request.append(sp500Stocks[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now let's loop to get the data, but let's give the API some time to receive the data, so we don't overwhelm the system with a request.\n",
    "\n",
    "import time\n",
    "from random import randint\n",
    "\n",
    "for ticker in data_request:\n",
    "       \n",
    "    data, meta_data = ts.get_intraday(symbol= ticker , outputsize='full')\n",
    "    \n",
    "    #Now we have to create a filename dynamically, so we will use string manipulation\n",
    "    \n",
    "    filename = ticker +'.csv'\n",
    "    \n",
    "    data.to_csv(filename)\n",
    "    #API limits you to 5 API calls per minute\n",
    "    time.sleep(randint(1,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('MMM.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
